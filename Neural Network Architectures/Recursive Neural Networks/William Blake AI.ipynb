{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project: Generating William Blake's Verses with Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "1. Read File\n",
    "2. Split lines\n",
    "3. Remove undesired lines (empty lines, titles, the beginning of the file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter(X: List[str]) -> List[str]:\n",
    "    '''\n",
    "    1. Creates a list of filter functions to remove undesired strings.\n",
    "    2. Apply recursively each filter in the list X of strings, emptying the list of filters.\n",
    "    3. Returns a list of filtered strings.\n",
    "    '''\n",
    "    filters = [lambda x: x != '',\n",
    "               lambda x: not x.isupper(),\n",
    "               lambda x: not ('    ' in x)]\n",
    "    \n",
    "    def apply_filters(x: str, _filters: List[Callable]) -> str:\n",
    "        return apply_filters(list(filter(_filters.pop(), x)), _filters) if len(_filters) > 1 else list(filter(_filters.pop(), x))\n",
    "    \n",
    "    return apply_filters(X, filters)\n",
    "\n",
    "def replace_all(X: str, replace_list: List[str]) -> str:\n",
    "    return replace_all(X.replace(replace_list.pop(), ''), replace_list) if len(replace_list) > 0 else X\n",
    "\n",
    "def substitute(Y: List[str], X: List[str]) -> List[str]:\n",
    "    return substitute(Y + [replace_all(X.pop(), ['   ', ',', '.', ':', ';', '—', '‘', '”', '’', '-', ')', '('])], X) if len(X) > 0 else Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And wish to lead others when they should be led',\n",
       " 'And feelthey know not what but care',\n",
       " 'They stumble all night over bones of the dead',\n",
       " 'How many have fallen there!',\n",
       " 'Tangled roots perplex her ways']"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open(\"/home/roboto/Documents/GitHub/tutorials/data/WilliamBlake.txt\", 'r').read()\n",
    "    # 1. Get splitted lines from the original files\n",
    "    # 2. Filter lines with the '_filter' function\n",
    "    # 3. Remove the first 13 and last 257 which aren't verses\n",
    "    # 4. Proccess each verse to remove spaces, commas, etc. with the 'substitute' function (the returned list is reversed)\n",
    "data = substitute([], _filter(data.split('\\n'))[13:-257])\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Build dataset class\n",
    "5. Build dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_idx('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "all_letters = string.ascii_letters + '“éè!? '\n",
    "n_letters   = len(all_letters) + 1\n",
    "char_to_id = {idx:char for char, idx in enumerate(list(all_letters))}\n",
    "id_to_char = {char:idx for char, idx in enumerate(list(all_letters))}\n",
    "\n",
    "def encode(idx: int, vector_length: int = n_letters) -> torch.Tensor:\n",
    "    vec = torch.zeros(vector_length)\n",
    "    vec[idx] = 1\n",
    "    return vec\n",
    "\n",
    "def char_to_vector(char: str) -> torch.Tensor:\n",
    "    return encode(char_to_id[char])\n",
    "\n",
    "def vector_to_char(encoding: torch.Tensor) -> str:\n",
    "    return id_to_char[np.where(encoding == 1)[0].item()]\n",
    "\n",
    "def string_to_vector(string: str) -> torch.Tensor:\n",
    "    vector = []\n",
    "    for char in list(string):\n",
    "        vector.append(char_to_vector(char))\n",
    "        \n",
    "    return torch.stack(vector, 0)\n",
    "\n",
    "def vector_to_string(encoding: torch.Tensor) -> str:\n",
    "    '''\n",
    "    Encoding(batch_size, sentence_length, n_letters)\n",
    "    '''\n",
    "    strings = []\n",
    "    for i in range(encoding.shape[0]):\n",
    "        v = encoding[i, :, :]\n",
    "        strings.append(''.join([vector_to_char(v) for v in encoding.squeeze(0)]))\n",
    "    \n",
    "    return strings[0]\n",
    "#     return strings # not working with batches yet\n",
    "\n",
    "def string_to_idx(string: str) -> List[int]:\n",
    "    string = list(string)\n",
    "    indexes = []\n",
    "    for char in string:\n",
    "        indexes.append(char_to_id[char])\n",
    "        \n",
    "    return indexes\n",
    "\n",
    "def idx_to_string(indexes: List[int]) -> str:\n",
    "    return ''.join([id_to_char[idx] for idx in indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VersesDataset(Dataset):\n",
    "    def __init__(self, verses: List[str]):\n",
    "        super().__init__()\n",
    "        self.samples = []\n",
    "        self.generate_samples(verses)\n",
    "        \n",
    "    def generate_samples(self, verses: List[str]):\n",
    "        for v in verses:\n",
    "            _input = string_to_vector(v[:-1])\n",
    "            target = string_to_idx(v[1:])\n",
    "            self.samples.append((_input, target))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "               \n",
    "def build_data(data: List[str], batch_size: int = 1, test_split: float = 0.1,\n",
    "               shuffle = True, seed = 42):\n",
    "    dataset = VersesDataset(data)\n",
    "    indexes = random.sample(list(range(len(data))), len(data)) # creates a list of 'len(verses)' randomly ordered numbers\n",
    "    train_indexes, test_indexes = indexes[int(test_split*len(data)):], indexes[:int(test_split*len(data))]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indexes)\n",
    "    test_sampler  = SubsetRandomSampler(test_indexes)\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size = batch_size, sampler = train_sampler)\n",
    "    test_loader  = DataLoader(dataset, batch_size = batch_size, sampler = test_sampler)\n",
    "    \n",
    "    dataloaders = {'Train': train_loader,\n",
    "                   'Test': test_loader}\n",
    "    \n",
    "    return dataset, dataloaders\n",
    "\n",
    "tensor_to_int = lambda xs: [x.item() for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "[4, 0, 19, 4, 3, 57, 8, 13, 57, 2, 14, 12, 15, 0, 13, 8, 4, 18, 57, 19, 7, 4, 24, 57, 18, 8, 19, 57, 22, 8, 19, 7, 57, 17, 0, 3, 8, 0, 13, 2, 4, 57, 0, 11, 11, 57, 19, 7, 4, 8, 17, 57, 14, 22, 13]\n"
     ]
    }
   ],
   "source": [
    "dataset, dataloaders = build_data(data)\n",
    "# It's not possible yet to build dataloaders with > 1 batches\n",
    "# I'm too lazy to work on that now\n",
    "train_dataloader = dataloaders['Train']\n",
    "test_dataloader = dataloaders['Test']\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0])\n",
    "print(tensor_to_int(sample[1]))\n",
    "\n",
    "# TO-DO: build a method to transform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Study of Recursive Neural Networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
