{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 3 0 4 1]\n",
      " [5 7 4 5 1]]\n",
      "[[0 4 1 6 1]\n",
      " [5 7 2 5 8]]\n"
     ]
    }
   ],
   "source": [
    "text = \"this is a test text!\"\n",
    "chars = list(set(text))\n",
    "    # set(text): good way to obtain the unique elements of a string\n",
    "indexer = {char:index for (index, char) in enumerate(chars)}\n",
    "\n",
    "indexed_data = []\n",
    "for c in text:\n",
    "    indexed_data.append(indexer[c])\n",
    "    \n",
    "    \n",
    "# creating batches\n",
    "x = np.array(indexed_data).reshape((2, -1))\n",
    "    # reshape the array into a matrix of two rows and the necessary number of columns\n",
    "for b in range(0, x.shape[1], 5):\n",
    "    batch = x[:, b:b + 5]\n",
    "    print(batch)\n",
    "    # divide the sequences inside each batch in subsequences of 5 characters\n",
    "    \n",
    "# one-hot-encoding\n",
    "batch = np.array([[2, 4, 7, 6, 5],\n",
    "                  [2, 1, 6, 2, 5]])\n",
    "batch_flatten = batch.flatten() # return a copy of the array collapsed into one dimension\n",
    "onehot_flat = np.zeros((batch.shape[0]*batch.shape[1], len(indexer)))\n",
    "onehot_flat[range(len(batch_flatten)), batch_flatten] = 1\n",
    "    # for each element of the onehot_flat array (first dimension),\n",
    "    # select the correct indicator (second dimension) and set it to 1\n",
    "oneht = onehot_flat.reshape((batch.shape[0], batch.shape[1], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocessing the input data and creating a one-hot matrix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l': 0, 'H': 1, ' ': 2, 'r': 3, 'e': 4, 'W': 5, 'o': 6, 'd': 7, '!': 8}\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello World!\"\n",
    "\n",
    "chars = list(set(text))\n",
    "indexer = {char:index for (index, char) in enumerate(chars)}\n",
    "print(indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4 0 0 6 2]\n",
      " [5 6 3 0 7 8]]\n"
     ]
    }
   ],
   "source": [
    "encoded = []\n",
    "for c in text:\n",
    "    encoded.append(indexer[c])\n",
    "\n",
    "encoded = np.array(encoded).reshape(2, -1)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index2onehot(batch: np.ndarray):\n",
    "    batch_flatten = batch.flatten() # Return a copy of the array collapsed into one dimension.\n",
    "    onehot_flat = np.zeros((batch.shape[0] * batch.shape[1], len(indexer)))\n",
    "    # dimensions: (sequence_length, num_features)\n",
    "    onehot_flat[range(len(batch_flatten)), batch_flatten] = 1 # for the whole sequence, select the correspondent index given \n",
    "                                                              # by batch_flatten and set it to 1\n",
    "    onehot = onehot_flat.reshape((batch.shape[0], batch.shape[1], -1)) # rearrange the array to (num_samples, sequence_size, num_features)\n",
    "    \n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "one_hot = index2onehot(encoded)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Building the Architecture_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, char_length, hidden_size, n_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers    = n_layers\n",
    "        self.lstm = nn.LSTM(char_length, hidden_size, n_layers, batch_first = True)\n",
    "            # char length: The number of expected features in the input `x`\n",
    "            # hidden_size: The number of features in the hidden state\n",
    "            # Number of recurrent layers. E.g., setting ``num_layers=2``\n",
    "                # would mean stacking two LSTMs together to form a `stacked LSTM`,\n",
    "                # with the second LSTM taking in outputs of the first LSTM and\n",
    "                # computing the final results.\n",
    "        self.output = nn.Linear(hidden_size, char_length) # Para cada batch (uma sequÃªncia independente)\n",
    "        \n",
    "    def forward(self, x, states):\n",
    "        out, states = self.lstm(x, states)\n",
    "        out = out.contiguous().view(-1, self.hidden_size) # Why this is rearranged this way?\n",
    "                                                          # Why not use the hidden state as input for the output layer?\n",
    "        out = self.output(out)\n",
    "            # why is it necessary to put another output layer? This one is the classifier?\n",
    "            # Yes, it must be the classifier.\n",
    "        \n",
    "        return out, states\n",
    "    \n",
    "    def init_states(self, batch_size):\n",
    "        hidden = next(self.parameters()).data.new(self.n_layers, batch_size, self.hidden_size).zero_()\n",
    "        cell   = next(self.parameters()).data.new(self.n_layers, batch_size, self.hidden_size).zero_()\n",
    "        # You must remember that the state is a vector, hence the necessity to create a matrix with the state vector for all batches.\n",
    "        # Why? I would need to understand how this is implemented.\n",
    "        states = (hidden, cell)\n",
    "        \n",
    "        return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Training the Model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Number of epochs\n",
    "for e in range(1, epochs + 1):\n",
    "    # Step 2: memory Initialized\n",
    "    states = model.init_states(n_seq) # Couldn't i put this inside the model?\n",
    "    \n",
    "    # Step 3: for loop to split data in batches.\n",
    "    for b in range(0, x.shape[1], seq_length): # for 0 to x.shape[1] (size of the whole sequence) by steps of seq_length\n",
    "        x_batch = x[:, b:b + seq_length]\n",
    "        \n",
    "        if b == x.shape[1] - seq_length: # for the last sequence\n",
    "            y_batch = x[:, b+1:b+seq_length]\n",
    "            y_batch = np.hstack((y_batch, indexer[\".\"]*np.ones((y_batch.shape[0], 1)))) # ?????????????\n",
    "        else: # for the earlier sequences\n",
    "            y_batch = x[:, b+1: b+seq_length+1]\n",
    "        \n",
    "        # Step 4: input data is converted to one-hot matrix. Inputs and targets are converted to tensors\n",
    "        \n",
    "        x_onehot = torch.Tensor(index2onehot(x_batch))\n",
    "        y = torch.Tensor(y_batch).view(n_seq * seq_length)\n",
    "        \n",
    "        # Step 5: get a prediction and perform the backward propagation.\n",
    "        \n",
    "        pred, states = model(x_onehot, states)\n",
    "        loss = loss_function(pred, y.long())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
