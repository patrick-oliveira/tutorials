{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "confident-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-artist",
   "metadata": {},
   "source": [
    "### Dice Loss\n",
    "***\n",
    "The Dice coefficient, or Dice-SÃ¸rensen coefficient, is a common metric for pixel segmentation that can also be modified to act as a loss function:\n",
    "\n",
    "$$\n",
    "DSC = \\frac{2|X\\cap Y|}{|X| + |Y|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sticky-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diceCoefficient(X: torch.tensor, Y: torch.tensor, smooth: float = 1.) -> torch.float:\n",
    "    '''\n",
    "    Inputs: It's expected that X and Y are tensors of 0s and 1s.\n",
    "    '''\n",
    "    return (2.*(X*Y).sum() + smooth) / (X.sum() + targets.sum() + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thirty-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight = None, size_average = True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, inputs, targets, smooth = 1):\n",
    "        inputs = F.sigmoid(inputs) # remove if model contains a sigmoid activation layer\n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice         = diceCoefficient(inputs, targets)\n",
    "        \n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-picnic",
   "metadata": {},
   "source": [
    "### BCE-Dice Loss\n",
    "***\n",
    "\n",
    "This loss combines Dice loss with the standard binary cross-entropy (BCE) loss that is generally the default for segmentation models. Combining the two methods allows for some diversity in the loss, while benefitting from the stability of BCE. The equation for multi-class BCE by itself will be familiar to anyone who has studied logistic regression:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = \\frac{1}{N} \\sum_{n = 1}^{N} H(p_n, q_n) = - \\frac{1}{N} \\sum_{n = 1}^{N} \\left[ y_n \\log \\hat{y}_n + (1 - y_n) \\log (1 - \\hat{y}_n \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "advanced-aurora",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.loss.MSELoss"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight = None, size_average = None):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "        \n",
    "    def forward(self, inputs, targets, smooth = 1):\n",
    "        inputs = F.sigmoid(inputs) # remove if model contains a sigmoid activation layer\n",
    "        \n",
    "        # flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice_loss    = 1 - diceCoefficient(inputs, targets)\n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction = 'mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "        \n",
    "        return Dice_BCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-commons",
   "metadata": {},
   "source": [
    "### Jaccard/Intersection over Union (IoU) Loss\n",
    "***\n",
    "The IoU metric, or Jaccard Index, is similar to the Dice metric and is calculated as the ratio between the overlap of the positive instances between two sets, and their mutual combined values:\n",
    "\n",
    "$$\n",
    "J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|} = \\frac{|A \\cap B|}{|A| + |B| - |A\\cap B|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "descending-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, weight = None, size_average = True):\n",
    "        super(IoULoss, self).__init__()\n",
    "    \n",
    "    def forward(self, inputs, targets, smooth = 1):\n",
    "        inputs = F.sigmoid(inputs) # remove if model contains a sigmoid activation layer\n",
    "        \n",
    "        # flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # intersection is equivalent to True Positive count\n",
    "        # union is the mutually inclusive area of all labels & predictions\n",
    "        intersection = (inputs * targets).sum()\n",
    "        total = (inputs + targets).sum()\n",
    "        union = total - intersection\n",
    "        \n",
    "        IoU = (intersection + smooth)/(union + smooth)\n",
    "        \n",
    "        return 1- IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-tourist",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
